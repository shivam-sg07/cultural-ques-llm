{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd8dcbed-cbf7-4490-afe8-94645586cc66",
   "metadata": {},
   "source": [
    "# Load required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1952959b-949f-4a47-9aaf-0291aa537289",
   "metadata": {},
   "source": [
    "To install the packages required for this notebook on the HPC, please follow the 'Jupyter Kernel Creation' slides posted on OPAL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a92f404-8c4f-4c8f-94fb-fcf3a25df16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/horse/ws/gadh722g-llm_project/llm_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f37a06-0f9d-4a48-ba11-db02b031a9e5",
   "metadata": {},
   "source": [
    "# Load the model (Llama-8B or Mistral-7B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74a6300-b87d-4b5c-a9c1-1deb92764c99",
   "metadata": {},
   "source": [
    "Note that you need to be on the partition with GPU (e.g. capella, alpha)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8e853e-56db-406a-a80e-3654603f9efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3841159-8914-4ef6-b145-936c434568b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"YOUR_HF_TOKEN_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17af3984-71f5-44d7-a1d7-11dd828e867b",
   "metadata": {},
   "source": [
    "This is the model which doesn't require requesting access. If you have the access to the Llama-8B model, you can use it instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed20d1a0-8483-453a-b4c9-2bf878390514",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f414f9-3127-48a1-9359-8a98cf0d6a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 4 files: 100%|██████████| 4/4 [01:18<00:00, 19.74s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [02:16<00:00, 34.05s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0858af5b-e505-4824-9522-9ad069d0f34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "saq = pd.read_csv(\"test_dataset_saq.csv\")\n",
    "saq = saq[[\"ID\", \"en_question\", \"country\"]]\n",
    "\n",
    "COUNTRY_MAP = {\n",
    "    'IR': 'Iran',\n",
    "    'CN': 'China',\n",
    "    'US': 'United States',\n",
    "    'GB': 'United Kingdom', \n",
    "    'UK': 'United Kingdom' \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3032ed6f-9a93-41b5-a02b-00b7f64af9c4",
   "metadata": {},
   "source": [
    "## With few shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad429b72-0fcf-4365-90fe-12ac70879584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting baseline generation with strict cleaning...\n",
      "[Iran] What is the most popular children's anim... -> tom\n",
      "[United States] Which country is considered the biggest ... -> mexico\n",
      "[United Kingdom] What is the duration (in hours) of a typ... -> 8\n",
      "[China] What is the most popular women's sports ... -> women's volleyball\n",
      "[United Kingdom] What are the family-related holidays in ... -> christmas\n",
      "[United States] What do farmers in US typically wear to ... -> cowboy hat\n",
      "[Iran] How many school breaks are there in a ye... -> 3\n",
      "[China] Who is the most famous Paralympian in Ch... -> zhang lixin\n",
      "[United States] What sports do male students enjoy durin... -> football\n",
      "[China] Which cities or regions are known for th... -> shanghai\n",
      "[Iran] What is the most common spice/herb used ... -> saffron\n",
      "[United States] In US, how long (in years) does a Master... -> 2\n",
      "[United States] What traditional games do families play ... -> football\n",
      "[United Kingdom] Which subject’s academy/private educatio... -> drama\n",
      "[Iran] In Iran, how long (in years) does a Mast... -> 2\n",
      "[United States] What is the most commonly learned musica... -> piano\n",
      "[United Kingdom] What sports do men like to play the most... -> football\n",
      "[China] What is the most popular music genre amo... -> pop\n",
      "[United Kingdom] At what age do children typically become... -> 18\n",
      "[China] In the Olympic Games, which sport is the... -> table tennis\n",
      "[United States] What is the common symbol of New Year's ... -> ball drop\n",
      "[United States] What carbohydrate is usually served with... -> french fries\n",
      "[United Kingdom] At what age do most people start working... -> 16\n",
      "[United Kingdom] What is the most famous alcohol brand in... -> guinness\n",
      "[Iran] Which region of Iran is well known for i... -> khorasan\n",
      "[Iran] What is the most popular indoor sport in... -> volleyball\n",
      "[Iran] What traditional games do families play ... -> backgammon\n",
      "[United Kingdom] At what time do most people start work i... -> 09:00\n",
      "[United Kingdom] Where do university students in UK usual... -> library\n",
      "[China] In which sport has China been most succe... -> table tennis\n",
      "[United States] What is the most popular traditional des... -> apple pie\n",
      "[Iran] Who is the most popular winter sports pl... -> alireza faghani\n",
      "[Iran] What is the most famous private company ... -> saipa\n",
      "[United Kingdom] What is the most commonly learned musica... -> piano\n",
      "Generation complete.\n",
      "Saved to saq_prediction_baseline_strict.tsv\n"
     ]
    }
   ],
   "source": [
    "def clean_answer_strict(text):\n",
    "    # 1. Isolate the final answer (just in case the model is chatty)\n",
    "    if \"Answer:\" in text:\n",
    "        text = text.split(\"Answer:\")[-1]\n",
    "    \n",
    "    # 2. Normalize\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # 3. CUT LISTS (Crucial for Instructor's New Rule)\n",
    "    # If the output is \"tennis, football, golf\" -> keeps only \"tennis\"\n",
    "    if ',' in text:\n",
    "        text = text.split(',')[0]\n",
    "    \n",
    "    # If the output is \"walking and jogging\" -> keeps only \"walking\"\n",
    "    if ' and ' in text:\n",
    "        text = text.split(' and ')[0]\n",
    "        \n",
    "    # 4. Handle \"Chatty\" Refusals\n",
    "    # If the model starts rambling \"i'm not sure...\", force it to be \"unknown\"\n",
    "    if \"not sure\" in text or \"i think\" in text:\n",
    "        return \"unknown\"\n",
    "\n",
    "    # 5. Remove terminal punctuation (.,!) only\n",
    "    if text and text[-1] in string.punctuation:\n",
    "        text = text[:-1]\n",
    "    \n",
    "    # 6. Remove articles\n",
    "    text = re.sub(r'^(the|a|an)\\s+', '', text)\n",
    "    \n",
    "    # 7. Final Safety\n",
    "    return text.split('\\n')[0].strip()\n",
    "\n",
    "def saq_base_func(question: str, country_code: str):\n",
    "    # Fallback: If code is not in map, use the code itself\n",
    "    country_name = COUNTRY_MAP.get(country_code, country_code)\n",
    "    \n",
    "    prompt = f\"\"\"Task: Answer the question with a single entity, name, or number.\n",
    "\n",
    "Question: What is the most popular sport?\n",
    "Country: Brazil\n",
    "Answer: football\n",
    "\n",
    "Question: At what age do students usually graduate high school?\n",
    "Country: United States\n",
    "Answer: 18\n",
    "\n",
    "Question: What is the traditional morning drink?\n",
    "Country: United Kingdom\n",
    "Answer: tea\n",
    "\n",
    "Question: {question}\n",
    "Country: {country_name}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=8,   # Kept strict limit\n",
    "            do_sample=False,    # Deterministic\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated = tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # USING THE STRICT CLEANER HERE\n",
    "    return clean_answer_strict(generated)\n",
    "\n",
    "\n",
    "preds = []\n",
    "\n",
    "print(\"Starting baseline generation with strict cleaning...\")\n",
    "\n",
    "for index, row in saq.iterrows():\n",
    "    q = row['en_question']\n",
    "    c = row['country']\n",
    "    \n",
    "    answer = saq_base_func(q, c)\n",
    "    preds.append(answer)\n",
    "    \n",
    "    # Print every 20th row to check\n",
    "    if index % 20 == 0:\n",
    "        c_name = COUNTRY_MAP.get(c, c)\n",
    "        print(f\"[{c_name}] {q[:40]}... -> {answer}\")\n",
    "\n",
    "saq[\"answer\"] = preds\n",
    "print(\"Generation complete.\")\n",
    "\n",
    "saq_submission = saq[[\"ID\", \"answer\"]]\n",
    "saq_submission.to_csv(\"saq_prediction_baseline_strict.tsv\", sep='\\t', index=False)\n",
    "print(\"Saved to saq_prediction_baseline_strict.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30625446-e851-4175-a1e5-0e0d2b5462c4",
   "metadata": {},
   "source": [
    "# Self Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25dc6b1-5429-4d03-932b-df1bf3886082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def saq_sc_func(question: str, country_code: str, n_samples: int = 5):\n",
    "    # Fallback: If code is not in map, use the code itself\n",
    "    country_name = COUNTRY_MAP.get(country_code, country_code)\n",
    "    \n",
    "    prompt = f\"\"\"Task: Answer the question with a single entity, name, or number.\n",
    "\n",
    "Question: What is the most popular sport?\n",
    "Country: Brazil\n",
    "Answer: football\n",
    "\n",
    "Question: At what age do students usually graduate high school?\n",
    "Country: United States\n",
    "Answer: 18\n",
    "\n",
    "Question: What is the traditional morning drink?\n",
    "Country: United Kingdom\n",
    "Answer: tea\n",
    "\n",
    "Question: {question}\n",
    "Country: {country_name}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=8,\n",
    "            do_sample=True,          # Enabled sampling\n",
    "            temperature=0.7,         # Variance control\n",
    "            top_p=0.9,               # Nucleus sampling\n",
    "            num_return_sequences=n_samples, \n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Extract and clean all generated samples\n",
    "    all_responses = []\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        generated_text = tokenizer.decode(outputs[i][input_len:], skip_special_tokens=True)\n",
    "        cleaned = clean_answer_strict(generated_text)\n",
    "        all_responses.append(cleaned)\n",
    "    \n",
    "    # Majority Voting\n",
    "    # Counter.most_common(1) returns [('answer', count)]\n",
    "    vote_counts = Counter(all_responses)\n",
    "    final_answer = vote_counts.most_common(1)[0][0]\n",
    "    \n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d8e7ca-7e66-421f-bc2c-62fcea776fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_sc = []\n",
    "\n",
    "print(f\"Starting Self-Consistency (N=5) generation...\")\n",
    "\n",
    "for index, row in saq.iterrows():\n",
    "    q = row['en_question']\n",
    "    c = row['country']\n",
    "    \n",
    "    # Using the SC function\n",
    "    answer = saq_sc_func(q, c, n_samples=5)\n",
    "    preds_sc.append(answer)\n",
    "    \n",
    "    if index % 20 == 0:\n",
    "        c_name = COUNTRY_MAP.get(c, c)\n",
    "        print(f\"[{c_name}] {q[:40]}... -> {answer}\")\n",
    "\n",
    "saq[\"answer\"] = preds_sc\n",
    "saq_submission = saq[[\"ID\", \"answer\"]]\n",
    "saq_submission.to_csv(\"saq_prediction_sc_n5.tsv\", sep='\\t', index=False)\n",
    "print(\"Saved to saq_prediction_sc_n5.tsv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_venv",
   "language": "python",
   "name": "llm_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
